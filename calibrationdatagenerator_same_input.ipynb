{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKZ124PJzMas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca262c6-0760-413c-972f-c18ef3056b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dishwasher': 0, 'toilet_tm': 0, 'sink_tm': 0, 'toilet_km': 0, 'shower_tm': 0, 'shower_km': 0, 'hose': 0, 'laundry': 0}\n"
          ]
        }
      ],
      "source": [
        "# Define a dictionary to hold appliance names as keys and corresponding data groups as values\n",
        "appliance_tuple = (\n",
        "    ([\"dishwasher\"], [\n",
        "        ('05/04/2024 23:19:28', '01410593'),\n",
        "        ('05/04/2024 23:19:38', '01410594'),\n",
        "        ('05/04/2024 23:19:49', '01410599'),\n",
        "        ('05/04/2024 23:19:59', '01410604'),\n",
        "        ('05/04/2024 23:20:10', '01410606'),\n",
        "        ('05/04/2024 23:20:20', '01410610'),\n",
        "        ('05/04/2024 23:20:31', '01410611'),\n",
        "        ('05/04/2024 23:20:52', '01410612'),\n",
        "        ('05/04/2024 23:21:54', '01410612'),\n",
        "        ('05/04/2024 23:22:26', '01410613'),\n",
        "        ('05/04/2024 23:23:08', '01410614'),\n",
        "        ('05/04/2024 23:23:18', '01410616'),\n",
        "        ('05/04/2024 23:23:28', '01410618'),\n",
        "        ('05/04/2024 23:23:39', '01410620'),\n",
        "        ('05/04/2024 23:23:50', '01410622'),\n",
        "        ('05/04/2024 23:24:00', '01410624'),\n",
        "        ('05/04/2024 23:24:11', '01410625'),\n",
        "        ('05/04/2024 23:24:42', '01410628'),\n",
        "        ('05/04/2024 23:24:52', '01410632'),\n",
        "        ('05/04/2024 23:25:03', '01410634'),\n",
        "        ('05/04/2024 23:29:25', '01410636'),\n",
        "        ('05/04/2024 23:29:35', '01410638'),\n",
        "        ('05/04/2024 23:29:45', '01410640'),\n",
        "        ('05/04/2024 23:29:56', '01410642'),\n",
        "        ('05/04/2024 23:30:06', '01410644'),\n",
        "        ('05/05/2024 00:26:59', '01410645'),\n",
        "        ('05/05/2024 00:27:52', '01410646'),\n",
        "        ('05/05/2024 00:28:44', '01410647'),\n",
        "        ('05/05/2024 00:28:55', '01410651'),\n",
        "        ('05/05/2024 00:29:05', '01410659'),\n",
        "        ('05/05/2024 00:29:15', '01410661'),\n",
        "        ('05/05/2024 00:29:47', '01410662'),\n",
        "        ('05/05/2024 00:30:39', '01410663'),\n",
        "        ('05/05/2024 00:31:31', '01410664'),\n",
        "        ('05/05/2024 00:32:24', '01410665'),\n",
        "        ('05/05/2024 00:33:16', '01410666'),\n",
        "        ('05/05/2024 00:34:09', '01410667'),\n",
        "        ('05/05/2024 00:34:40', '01410669'),\n",
        "        ('05/05/2024 00:34:51', '01410671'),\n",
        "        ('05/05/2024 00:35:01', '01410673'),\n",
        "        ('05/05/2024 00:35:12', '01410675'),\n",
        "        ('05/05/2024 00:35:22', '01410678'),\n",
        "        ('05/05/2024 00:35:33', '01410679'),\n",
        "        ('05/05/2024 00:36:36', '01410680'),\n",
        "        ('05/05/2024 00:37:28', '01410681'),\n",
        "        ('05/05/2024 00:38:20', '01410682'),\n",
        "        ('05/05/2024 00:39:13', '01410683'),\n",
        "        ('05/05/2024 00:40:16', '01410684'),\n",
        "        ('05/05/2024 00:41:08', '01410685'),\n",
        "        ('05/05/2024 00:42:00', '01410686'),\n",
        "        ('05/05/2024 00:42:53', '01410687'),\n",
        "        ('05/05/2024 00:43:34', '01410688'),\n",
        "        ('05/05/2024 00:43:45', '01410690'),\n",
        "        ('05/05/2024 00:43:56', '01410693'),\n",
        "        ('05/05/2024 00:44:06', '01410695'),\n",
        "        ('05/05/2024 00:44:17', '01410697'),\n",
        "        ('05/05/2024 00:44:27', '01410698'),\n",
        "        ('05/05/2024 00:44:38', '01410699'),\n",
        "        ('05/05/2024 00:45:41', '01410700'),\n",
        "        ('05/05/2024 01:54:05', '01410707'),\n",
        "        ('05/05/2024 01:54:15', '01410712')\n",
        "    ]),\n",
        "\n",
        "    ([\"toilet_tm\"], [\n",
        "        ('05/05/2024 11:52:26', '01410818'),\n",
        "        ('05/05/2024 11:52:36', '01410819'),\n",
        "        ('05/05/2024 11:52:47', '01410826'),\n",
        "        ('05/05/2024 11:52:57', '01410833'),\n",
        "    ]),\n",
        "\n",
        "    ([\"sink_tm\"], [\n",
        "        ('05/05/2024 11:56:48', '01410833'),\n",
        "        ('05/05/2024 11:56:58', '01410834'),\n",
        "    ]),\n",
        "\n",
        "    ([\"toilet_km\"], [\n",
        "        ('05/05/2024 12:05:00', '01410834'),\n",
        "        ('05/05/2024 12:05:10', '01410835'),\n",
        "        ('05/05/2024 12:05:21', '01410839'),\n",
        "        ('05/05/2024 12:05:31', '01410845'),\n",
        "        ('05/05/2024 12:05:42', '01410848')\n",
        "    ]),\n",
        "\n",
        "    ([\"shower_tm\"], [\n",
        "        ('05/05/2024 12:24:12', '01411062'),\n",
        "        ('05/05/2024 12:24:22', '01411063'),\n",
        "        ('05/05/2024 12:24:33', '01411064'),\n",
        "        ('05/05/2024 12:24:43', '01411065'),\n",
        "        ('05/05/2024 12:24:54', '01411066'),\n",
        "        ('05/05/2024 12:25:04', '01411067'),\n",
        "        ('05/05/2024 12:25:15', '01411068')\n",
        "    ]),\n",
        "\n",
        "    ([\"shower_km\"], [\n",
        "        ('05/05/2024 12:50:23', '01411084'),\n",
        "        ('05/05/2024 12:50:33', '01411086'),\n",
        "        ('05/05/2024 12:50:44', '01411088'),\n",
        "        ('05/05/2024 12:50:54', '01411090'),\n",
        "        ('05/05/2024 12:51:05', '01411093'),\n",
        "        ('05/05/2024 12:51:15', '01411095'),\n",
        "        ('05/05/2024 12:51:25', '01411097'),\n",
        "        ('05/05/2024 12:51:36', '01411099'),\n",
        "        ('05/05/2024 12:51:47', '01411101'),\n",
        "        ('05/05/2024 12:51:57', '01411103'),\n",
        "        ('05/05/2024 12:52:08', '01411105'),\n",
        "        ('05/05/2024 12:52:18', '01411107'),\n",
        "        ('05/05/2024 12:52:29', '01411109'),\n",
        "        ('05/05/2024 12:52:39', '01411111'),\n",
        "        ('05/05/2024 12:52:49', '01411113'),\n",
        "        ('05/05/2024 12:53:00', '01411115'),\n",
        "        ('05/05/2024 12:53:10', '01411117'),\n",
        "        ('05/05/2024 12:53:21', '01411119'),\n",
        "        ('05/05/2024 12:53:31', '01411121'),\n",
        "        ('05/05/2024 12:53:42', '01411122'),\n",
        "        ('05/05/2024 12:53:52', '01411124'),\n",
        "        ('05/05/2024 12:54:03', '01411126'),\n",
        "        ('05/05/2024 12:54:13', '01411128'),\n",
        "        ('05/05/2024 12:54:24', '01411130'),\n",
        "        ('05/05/2024 12:54:34', '01411132'),\n",
        "        ('05/05/2024 12:54:45', '01411134'),\n",
        "        ('05/05/2024 12:54:55', '01411136'),\n",
        "        ('05/05/2024 12:55:06', '01411137'),\n",
        "        ('05/05/2024 12:55:16', '01411139'),\n",
        "        ('05/05/2024 12:55:27', '01411141'),\n",
        "        ('05/05/2024 12:55:37', '01411143'),\n",
        "        ('05/05/2024 12:55:47', '01411145'),\n",
        "        ('05/05/2024 12:55:58', '01411147'),\n",
        "        ('05/05/2024 12:56:08', '01411148'),\n",
        "        ('05/05/2024 12:56:19', '01411150'),\n",
        "        ('05/05/2024 12:56:29', '01411152'),\n",
        "        ('05/05/2024 12:56:40', '01411154'),\n",
        "        ('05/05/2024 12:56:50', '01411156'),\n",
        "        ('05/05/2024 12:57:01', '01411157'),\n",
        "        ('05/05/2024 12:57:11', '01411159'),\n",
        "        ('05/05/2024 12:57:22', '01411161'),\n",
        "        ('05/05/2024 12:57:32', '01411163'),\n",
        "        ('05/05/2024 12:57:43', '01411165'),\n",
        "        ('05/05/2024 12:57:53', '01411166'),\n",
        "        ('05/05/2024 12:58:04', '01411168'),\n",
        "        ('05/05/2024 12:58:14', '01411170'),\n",
        "        ('05/05/2024 12:58:25', '01411172'),\n",
        "        ('05/05/2024 12:58:35', '01411174'),\n",
        "        ('05/05/2024 12:58:46', '01411175'),\n",
        "        ('05/05/2024 12:58:56', '01411177'),\n",
        "        ('05/05/2024 12:59:06', '01411179'),\n",
        "        ('05/05/2024 12:59:17', '01411181'),\n",
        "        ('05/05/2024 12:59:27', '01411182'),\n",
        "        ('05/05/2024 12:59:38', '01411184'),\n",
        "        ('05/05/2024 12:59:49', '01411186'),\n",
        "        ('05/05/2024 12:59:59', '01411188'),\n",
        "        ('05/05/2024 13:00:10', '01411190'),\n",
        "        ('05/05/2024 13:00:20', '01411191'),\n",
        "        ('05/05/2024 13:00:31', '01411193'),\n",
        "        ('05/05/2024 13:00:41', '01411195'),\n",
        "        ('05/05/2024 13:00:52', '01411197'),\n",
        "        ('05/05/2024 13:01:02', '01411198'),\n",
        "        ('05/05/2024 13:01:12', '01411200'),\n",
        "        ('05/05/2024 13:01:23', '01411202'),\n",
        "        ('05/05/2024 13:01:33', '01411204'),\n",
        "        ('05/05/2024 13:01:44', '01411205'),\n",
        "        ('05/05/2024 13:01:55', '01411207'),\n",
        "        ('05/05/2024 13:02:05', '01411208')\n",
        "    ]),\n",
        "\n",
        "    ([\"hose\"], [\n",
        "        ('05/05/2024 12:13:44', '01410862'),\n",
        "        ('05/05/2024 12:13:54', '01410870'),\n",
        "        ('05/05/2024 12:14:05', '01410878'),\n",
        "        ('05/05/2024 12:14:15', '01410887'),\n",
        "        ('05/05/2024 12:14:25', '01410895'),\n",
        "        ('05/05/2024 12:14:36', '01410903'),\n",
        "        ('05/05/2024 12:14:46', '01410911'),\n",
        "        ('05/05/2024 12:14:57', '01410920'),\n",
        "        ('05/05/2024 12:15:07', '01410928'),\n",
        "        ('05/05/2024 12:15:18', '01410936'),\n",
        "        ('05/05/2024 12:15:28', '01410945'),\n",
        "        ('05/05/2024 12:15:39', '01410953'),\n",
        "        ('05/05/2024 12:15:49', '01410961'),\n",
        "        ('05/05/2024 12:16:00', '01410970'),\n",
        "        ('05/05/2024 12:16:10', '01410978'),\n",
        "        ('05/05/2024 12:16:21', '01410986'),\n",
        "        ('05/05/2024 12:16:31', '01410994'),\n",
        "        ('05/05/2024 12:16:42', '01411003'),\n",
        "        ('05/05/2024 12:16:52', '01411011'),\n",
        "        ('05/05/2024 12:17:03', '01411014'),\n",
        "        ('05/05/2024 12:17:13', '01411021'),\n",
        "        ('05/05/2024 12:17:23', '01411029'),\n",
        "        ('05/05/2024 12:17:34', '01411037'),\n",
        "        ('05/05/2024 12:17:45', '01411046'),\n",
        "        ('05/05/2024 12:17:55', '01411054'),\n",
        "        ('05/05/2024 12:18:06', '01411060')\n",
        "    ]),\n",
        "\n",
        "    ([\"laundry\"], [\n",
        "        ('05/05/2024 22:47:22', '01412061'),\n",
        "        ('05/05/2024 22:47:33', '01412066'),\n",
        "        ('05/05/2024 22:51:13', '01412067'),\n",
        "        ('05/05/2024 22:51:23', '01412068'),\n",
        "        ('05/05/2024 22:51:34', '01412069'),\n",
        "        ('05/05/2024 22:51:44', '01412070'),\n",
        "        ('05/05/2024 22:51:55', '01412071'),\n",
        "        ('05/05/2024 22:52:05', '01412072'),\n",
        "        ('05/05/2024 22:52:16', '01412073'),\n",
        "        ('05/05/2024 22:52:37', '01412074'),\n",
        "        ('05/05/2024 22:52:47', '01412075'),\n",
        "        ('05/05/2024 22:52:58', '01412076'),\n",
        "        ('05/05/2024 22:53:08', '01412077'),\n",
        "        ('05/05/2024 22:53:29', '01412081'),\n",
        "        ('05/05/2024 22:53:40', '01412086'),\n",
        "        ('05/05/2024 22:53:50', '01412087'),\n",
        "        ('05/05/2024 22:58:54', '01412091'),\n",
        "        ('05/05/2024 22:59:04', '01412094'),\n",
        "        ('05/05/2024 22:59:15', '01412099'),\n",
        "        ('05/05/2024 22:59:25', '01412103'),\n",
        "        ('05/05/2024 22:59:36', '01412108'),\n",
        "        ('05/05/2024 22:59:46', '01412112'),\n",
        "        ('05/05/2024 22:59:56', '01412116'),\n",
        "        ('05/05/2024 23:00:07', '01412121'),\n",
        "        ('05/05/2024 23:00:17', '01412125'),\n",
        "        ('05/05/2024 23:00:28', '01412130'),\n",
        "        ('05/05/2024 23:00:38', '01412135'),\n",
        "        ('05/05/2024 23:00:49', '01412136'),\n",
        "        ('05/05/2024 23:00:59', '01412138'),\n",
        "        ('05/05/2024 23:01:10', '01412142'),\n",
        "        ('05/05/2024 23:01:20', '01412143'),\n",
        "        ('05/05/2024 23:01:31', '01412147'),\n",
        "        ('05/05/2024 23:01:41', '01412148'),\n",
        "        ('05/05/2024 23:04:18', '01412149'),\n",
        "        ('05/05/2024 23:04:29', '01412152'),\n",
        "        ('05/05/2024 23:04:39', '01412154'),\n",
        "        ('05/05/2024 23:05:00', '01412156'),\n",
        "        ('05/05/2024 23:05:21', '01412158'),\n",
        "        ('05/05/2024 23:05:31', '01412159'),\n",
        "        ('05/05/2024 23:05:42', '01412161'),\n",
        "        ('05/05/2024 23:09:22', '01412163'),\n",
        "        ('05/05/2024 23:09:32', '01412164'),\n",
        "        ('05/05/2024 23:09:43', '01412166'),\n",
        "        ('05/05/2024 23:09:53', '01412167'),\n",
        "        ('05/05/2024 23:10:04', '01412169'),\n",
        "        ('05/05/2024 23:10:14', '01412170'),\n",
        "        ('05/05/2024 23:10:56', '01412172'),\n",
        "        ('05/05/2024 23:15:39', '01412176'),\n",
        "        ('05/05/2024 23:15:49', '01412181'),\n",
        "        ('05/05/2024 23:16:00', '01412185'),\n",
        "        ('05/05/2024 23:16:10', '01412188'),\n",
        "        ('05/05/2024 23:16:20', '01412192'),\n",
        "        ('05/05/2024 23:16:31', '01412197'),\n",
        "        ('05/05/2024 23:16:41', '01412201'),\n",
        "        ('05/05/2024 23:16:52', '01412206'),\n",
        "        ('05/05/2024 23:17:02', '01412210'),\n",
        "        ('05/05/2024 23:17:13', '01412212'),\n",
        "        ('05/05/2024 23:19:08', '01412215'),\n",
        "        ('05/05/2024 23:19:18', '01412216'),\n",
        "        ('05/05/2024 23:20:21', '01412218'),\n",
        "        ('05/05/2024 23:20:32', '01412222'),\n",
        "        ('05/05/2024 23:20:42', '01412227'),\n",
        "        ('05/05/2024 23:20:52', '01412231'),\n",
        "        ('05/05/2024 23:21:03', '01412235'),\n",
        "        ('05/05/2024 23:21:13', '01412236'),\n",
        "        ('05/05/2024 23:21:24', '01412239'),\n",
        "        ('05/05/2024 23:21:34', '01412243'),\n",
        "        ('05/05/2024 23:21:45', '01412247'),\n",
        "        ('05/05/2024 23:21:55', '01412252'),\n",
        "        ('05/05/2024 23:24:01', '01412254'),\n",
        "        ('05/05/2024 23:24:12', '01412257'),\n",
        "        ('05/05/2024 23:24:22', '01412261'),\n",
        "        ('05/05/2024 23:24:32', '01412265'),\n",
        "        ('05/05/2024 23:24:43', '01412267'),\n",
        "        ('05/06/2024 00:25:59', '01412267'),\n",
        "        ('05/06/2024 01:26:11', '01412272'),\n",
        "        ('05/06/2024 01:26:21', '01412274')\n",
        "    ]),\n",
        ")\n",
        "\n",
        "\n",
        "appliance_info_dict = {'dishwasher': [0,False],\n",
        "                       \"toilet_tm\": [0, False],\n",
        "                       \"sink_tm\": [0, True],\n",
        "                       \"toilet_km\": [0, False],\n",
        "                       \"shower_tm\": [0, True],\n",
        "                       \"shower_km\": [0, True],\n",
        "                       \"hose\": [0, True],\n",
        "                       \"laundry\": [0, False]}\n",
        "\n",
        "appliance_gallon_dict = {key: value[0] for key, value in appliance_info_dict.items()}\n",
        "print(appliance_gallon_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2b8KoGYgHmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d3396e-0efc-4702-a148-5341a30daf1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7999999999883585, 0.7999999999883585, 0.9000000000232831, 0.7999999999883585, 0.7999999999883585, 0.8000000000174623, 0.8999999999941792, 0.7999999999883585, 0.8000000000174623, 0.8999999999941792, 0.7999999999883585, 0.8000000000174623, 0.8999999999941792, 0.7999999999883585, 0.8000000000174623, 0.7999999999883585, 0.8999999999941792, 0.8000000000174623, 0.29999999998835847, 0.7000000000116415, 0.7999999999883585, 0.8000000000174623, 0.8999999999941792, 0.7999999999883585, 0.6000000000058208, 0.7999999999883585, 0.8000000000174623, 0.7999999999883585, 0.8000000000174623, 0.7999999999883585, 0.8999999999941792, 0.7999999999883585, 0.7999999999883585, 0.7999999999883585, 0.7999999999883585, 0.8000000000174623, 0.7999999999883585, 0.7999999999883585, 0.29999999998835847, 0.7999999999883585, 0.8999999999941792, 0.8000000000174623, 0.7999999999883585, 0.7999999999883585, 0.7999999999883585, 0.8000000000174623, 0.7999999999883585, 0.6000000000058208, 0.7999999999883585, 0.8000000000174623, 0.7999999999883585, 0.7999999999883585, 0.6000000000058208, 0.8000000000174623, 0.9000000000232831, 0.9000000000232831, 0.9000000000232831, 0.8999999999941792, 0.8000000000174623, 0.7999999999883585, 0.8999999999941792, 0.8999999999941792, 0.7999999999883585, 0.9000000000232831, 0.7999999999883585, 0.6000000000058208, 0.7999999999883585, 0.7999999999883585, 0.29999999998835847, 0.8999999999941792, 0.7999999999883585, 0.8000000000174623, 0.8000000000174623, 0.8999999999941792, 0.8999999999941792, 0.7999999999883585, 0.7999999999883585, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import psutil\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "class CalibrationDataGenerator():\n",
        "\n",
        "    def __init__(self, appliance_info_dict, appliance_tuple1):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        appliance_info_dict (dict): {appliance_string_name: [gallons (int)), continuous flow/fixed flow (boolean)]}\n",
        "        appliance_tuple (tuple): (appliance_string_name: [tuples (seconds, water meter reading)]}\n",
        "\n",
        "        \"\"\"\n",
        "        self.appliance_gallon_dict = {key: value[0] for key, value in appliance_info_dict.items()}\n",
        "        self.appliance_boolean_continous_dict = {key: value[1] for key, value in appliance_info_dict.items()}\n",
        "\n",
        "        self.updated_calibrated_data = self.change_in_points(appliance_tuple1)\n",
        "\n",
        "\n",
        "    def remove_tuples_based_on_random_selection(self, random_tuple, tuple2):\n",
        "        first_values = set(random_tuple[0])\n",
        "\n",
        "        # Filter out tuples in tuple2 that have any of the first values in their first element\n",
        "        tuple2 = tuple(filter(lambda x: not any(item in first_values for item in x[0]), tuple2))\n",
        "\n",
        "        return tuple2\n",
        "\n",
        "    def create_full_second_lists(self, original_tuples, appliance_dict):\n",
        "\n",
        "        total_length = 91  # Total number of tuples in each list (for 90 seconds with 10-second intervals)\n",
        "        half_length = total_length // 2+1\n",
        "        padding_element = (10, 0)  # Default padding element\n",
        "\n",
        "        full_lists = []  # List to store the new full lists of tuples\n",
        "        outputs = []  # List to store the corresponding output lists\n",
        "\n",
        "        for i in range(len(original_tuples)):\n",
        "            # Initialize a list with padding elements\n",
        "            full_list = [padding_element] * total_length\n",
        "\n",
        "            # Fill the middle of the list with the original tuples\n",
        "            start_index = max(0, half_length - i)\n",
        "            end_index = min(total_length, half_length + len(original_tuples) - i)\n",
        "\n",
        "            for j in range(start_index, end_index):\n",
        "              full_list[j] = original_tuples[j - start_index]\n",
        "\n",
        "            # Create the corresponding output list\n",
        "            output_list = []\n",
        "            for appliance, values in appliance_dict.items():\n",
        "                if len(values) > i:\n",
        "                    output_list.append(values[i])\n",
        "                else:\n",
        "                    output_list.append(0)  # If the appliance list is shorter than the required length\n",
        "\n",
        "            # Add the full list and output list to the respective lists\n",
        "            full_lists.append(full_list)\n",
        "            outputs.append(output_list)\n",
        "\n",
        "        return full_lists, outputs\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "        inputs = []\n",
        "        outputs = []\n",
        "        xs, ys = self.create_points()\n",
        "\n",
        "        for x, y in zip(xs, ys):\n",
        "            input, output = self.create_full_second_lists(x,y)\n",
        "            for i, o in zip(input, output):\n",
        "                inputs.append([t[1] for t in i])\n",
        "                if sum(o) == 0:\n",
        "                  print(\"ERROR IS HAPPENING\")\n",
        "                outputs.append(list(o))\n",
        "\n",
        "\n",
        "        # Dictionary to count occurrences of each unique label configuration\n",
        "        label_counts = defaultdict(int)\n",
        "        label_inputs = {}\n",
        "        for k, output in enumerate(outputs):\n",
        "            label_tuple = tuple(output)\n",
        "            label_counts[label_tuple] += 1\n",
        "            if label_tuple not in label_inputs:\n",
        "                label_inputs[label_tuple] = []\n",
        "            label_inputs[label_tuple].append(inputs[k])\n",
        "\n",
        "\n",
        "        # Find the maximum count for unbias\n",
        "        max_counts = max(label_counts.values())\n",
        "\n",
        "        for item in label_counts.keys():\n",
        "          while label_counts[item] < max_counts:\n",
        "            inputs.append(random.choice(label_inputs[item]))\n",
        "            outputs.append(list(item))\n",
        "            label_counts[item] += 1\n",
        "\n",
        "\n",
        "        num_of_data_points = 10000\n",
        "\n",
        "        random_indexes = random.sample(range(len(inputs)), num_of_data_points)\n",
        "\n",
        "        inputs = [inputs[i] for i in random_indexes]\n",
        "        outputs = [outputs[i] for i in random_indexes]\n",
        "\n",
        "        return inputs, outputs\n",
        "\n",
        "\n",
        "    def create_points(self):\n",
        "        final_copy = self.updated_calibrated_data.copy()\n",
        "        x = []\n",
        "        y = []\n",
        "\n",
        "        # create extended appliances to x and y\n",
        "        num_of_extended_lists = 30\n",
        "\n",
        "        copy = self.updated_calibrated_data.copy()\n",
        "        for _ in range(num_of_extended_lists):\n",
        "            for (a, b, c) in copy:\n",
        "                if self.appliance_boolean_continous_dict[a[0]] == True:\n",
        "                    appliance, data, output = self.extend_usage_data((a, b))\n",
        "                    final_copy.append((appliance, data, output))\n",
        "\n",
        "        # add single and extended appliance to x and y\n",
        "        for (a, b, c) in final_copy:\n",
        "            x.append(b)\n",
        "            y.append(c)\n",
        "\n",
        "        # number of appliances\n",
        "        num_of_combined_lists = 100\n",
        "\n",
        "        for _ in range(num_of_combined_lists):\n",
        "            copy = self.updated_calibrated_data.copy()\n",
        "            number_of_appliances = random.randint(2, len(self.appliance_gallon_dict))\n",
        "\n",
        "            selected_tuples = []\n",
        "            for _ in range(number_of_appliances):\n",
        "                random_tuple = random.choice(copy)\n",
        "                selected_tuples.append(random_tuple)\n",
        "                copy = self.remove_tuples_based_on_random_selection(random_tuple, copy)\n",
        "\n",
        "            # Separate the selected tuples into three lists: appliances, data, and outputs\n",
        "            appliances = [tup[0][0] for tup in selected_tuples]\n",
        "            datas = [tup[1] for tup in selected_tuples]\n",
        "\n",
        "            appliance, data, output = self.combine_multiple_lists(appliances, datas)\n",
        "\n",
        "\n",
        "            #x.append(data)\n",
        "            #y.append(output)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "    def change_in_points(self, appliance_tuple):\n",
        "        \"\"\"\n",
        "        Calculate changes in water meter readings and timestamps for each appliance in the tuple.\n",
        "\n",
        "        :param appliance_tuple: tuple, containing multiple appliances and their usage data\n",
        "        :return: tuple, containing multiple appliances and their modified usage data\n",
        "        \"\"\"\n",
        "        modified_tuple = []\n",
        "\n",
        "        # Iterate over each appliance and its data\n",
        "        for appliance, data in appliance_tuple:\n",
        "            output = self.appliance_gallon_dict.copy()\n",
        "\n",
        "            # Extract timestamps and water meter readings\n",
        "            timestamps = [entry[0] for entry in data]\n",
        "            readings = [int(entry[1]) / 10.0 for entry in data]\n",
        "\n",
        "            # Calculate change in water meter reading\n",
        "            changes_readings = [readings[i + 1] - readings[i] for i in range(len(readings) - 1)]\n",
        "\n",
        "            # Calculate change in timestamps\n",
        "            changes_timestamps = [\n",
        "                (datetime.strptime(timestamps[i + 1], '%m/%d/%Y %H:%M:%S') - datetime.strptime(timestamps[i],\n",
        "                                                                                               '%m/%d/%Y %H:%M:%S')).total_seconds()\n",
        "                for i in range(len(timestamps) - 1)\n",
        "            ]\n",
        "\n",
        "            # Round down changes in timestamps to the nearest 10 seconds\n",
        "            rounded_changes_timestamps = [round(change / 10.0) * 10 for change in changes_timestamps]\n",
        "\n",
        "            # Combine timestamps and changes\n",
        "            combined_data = list(zip(rounded_changes_timestamps, changes_readings))\n",
        "\n",
        "            i = 0\n",
        "            while i < len(combined_data):\n",
        "                change_time, value = combined_data[i]\n",
        "                if change_time > 10:\n",
        "                    # Insert the modified tuples into the original_tuples list\n",
        "                    combined_data.insert(i + 1, (change_time - 10, value))\n",
        "                    combined_data[i] = (10, 0)\n",
        "\n",
        "                i += 1\n",
        "\n",
        "            for app in self.appliance_gallon_dict.keys():\n",
        "              if app == appliance[0]:\n",
        "                  output[app] = [1] * len(combined_data)\n",
        "              else:\n",
        "                  output[app] = [0] * len(combined_data)\n",
        "\n",
        "            modified_tuple.append((appliance, combined_data, output))\n",
        "\n",
        "        return modified_tuple\n",
        "\n",
        "    def combine_multiple_lists(self, items, lists):\n",
        "\n",
        "        combined_item = items[0]\n",
        "        combined_list = lists[0].copy()\n",
        "\n",
        "        output = self.appliance_gallon_dict.copy()\n",
        "\n",
        "        for app in self.appliance_gallon_dict.keys():\n",
        "            if app == combined_item:\n",
        "                output[app] = [1] * len(combined_list)\n",
        "            else:\n",
        "                output[app] = [0] * len(combined_list)\n",
        "\n",
        "        i = 0\n",
        "        while i < len(combined_list):\n",
        "            change_time, value = combined_list[i]\n",
        "            if change_time > 10:\n",
        "                # Insert the modified tuples into the original_tuples list\n",
        "                combined_list.insert(i + 1, (change_time - 10, value))\n",
        "                combined_list[i] = (10, 0)\n",
        "\n",
        "                # Insert corresponding values into the appliance_dict lists\n",
        "                for appliance in output:\n",
        "                    output[appliance].insert(i + 1, output[appliance][i])\n",
        "\n",
        "            i += 1\n",
        "\n",
        "\n",
        "        for i in range(1, len(lists)):\n",
        "            current_item = items[i]\n",
        "            current_list = lists[i]\n",
        "\n",
        "            i = 0\n",
        "            while i < len(current_list):\n",
        "                change_time, value = current_list[i]\n",
        "                if change_time > 10:\n",
        "                    # Insert the modified tuples into the original_tuples list\n",
        "                    current_list.insert(i + 1, (change_time - 10, value))\n",
        "                    current_list[i] = (10, 0)\n",
        "\n",
        "                i += 1\n",
        "\n",
        "\n",
        "            valid_indices = [j for j, (sec, _) in enumerate(combined_list) if sec == current_list[0][0]]\n",
        "\n",
        "            if not valid_indices:\n",
        "                raise ValueError(\"No valid insertion point found where the first elements match.\")\n",
        "\n",
        "            insert_index = random.choice(valid_indices)\n",
        "\n",
        "            for k, (sec, reading) in enumerate(current_list):\n",
        "                try:\n",
        "                    combined_sec, combined_reading = combined_list[insert_index + k]\n",
        "                    combined_list[insert_index + k] = (sec, combined_reading + reading)\n",
        "                except IndexError:\n",
        "                    combined_list.append((sec, reading))\n",
        "                    for app in self.appliance_gallon_dict.keys():\n",
        "                        output[app].append(0)\n",
        "\n",
        "            output[current_item][insert_index:insert_index + len(current_list)] = [1] * len(current_list)\n",
        "\n",
        "\n",
        "            combined_item += current_item\n",
        "\n",
        "        return combined_item, combined_list, output\n",
        "\n",
        "\n",
        "    def extend_usage_data(self, appliance_tuple):\n",
        "\n",
        "        appliance_name, usage_data = appliance_tuple\n",
        "\n",
        "        min_modifications = -len(usage_data) + 1\n",
        "        max_modifications = len(usage_data) * 10\n",
        "\n",
        "        output = self.appliance_gallon_dict.copy()\n",
        "\n",
        "        # Determine the number of points to add or remove randomly within the specified range\n",
        "        num_modifications = random.randint(min_modifications, max_modifications)\n",
        "\n",
        "        if num_modifications > 0:\n",
        "            # Add points\n",
        "            additional_data = [random.choice(usage_data) for _ in range(num_modifications)]\n",
        "            modified_data = usage_data + additional_data\n",
        "        else:\n",
        "            # Remove points, ensuring we don't remove more points than we have\n",
        "            if len(usage_data) + num_modifications <= 0:\n",
        "                modified_data = usage_data[:1]  # Keep at least one point\n",
        "            else:\n",
        "                modified_data = usage_data[:num_modifications]\n",
        "\n",
        "        for app in self.appliance_gallon_dict.keys():\n",
        "            if app == appliance_name[0]:\n",
        "                output[app] = [1] * len(modified_data)\n",
        "            else:\n",
        "                output[app] = [0] * len(modified_data)\n",
        "\n",
        "        return appliance_name, modified_data, output\n",
        "\n",
        "dg = CalibrationDataGenerator(appliance_info_dict, appliance_tuple)\n",
        "X, y = dg.get_data()\n",
        "print(X[0])\n",
        "print(y[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Masking\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, appliance_info_dict, appliance_tuple, batch_size=32, sequence_length=10, num_features=1, num_classes=8):\n",
        "        self.dg = CalibrationDataGenerator(appliance_info_dict, appliance_tuple)\n",
        "        self.inputs, self.outputs = self.dg.get_data()\n",
        "        self.batch_size = batch_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_features = num_features\n",
        "        self.num_classes = num_classes\n",
        "        self.count = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.inputs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.count % 1000 == 0:\n",
        "          self.inputs, self.outputs = self.dg.get_data()\n",
        "        self.count += 1\n",
        "\n",
        "        batch_inputs = self.inputs[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_outputs = self.outputs[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        X_temp = [np.array(x) for x in batch_inputs]\n",
        "        y_temp = [np.array(y) for y in batch_outputs]\n",
        "\n",
        "        #X_padded = pad_sequences(X_temp, maxlen=self.sequence_length, padding='post', dtype='float32', value=0.0)\n",
        "        #y_padded = pad_sequences(y_temp, maxlen=self.num_classes, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "        return np.array(X_temp), np.array(y_temp)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        indices = np.arange(len(self.inputs))\n",
        "        np.random.shuffle(indices)\n",
        "        self.inputs = [self.inputs[i] for i in indices]\n",
        "        self.outputs = [self.outputs[i] for i in indices]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the custom weighted loss function\n",
        "def custom_weighted_loss(y_true, y_pred):\n",
        "    # Define different weights for each class based on the label\n",
        "    weight_false_negative = 2.0  # Penalty for predicting 0 when it's actually 1\n",
        "    weight_true_positive = 1.0   # Reward for predicting 1 when it's actually 1\n",
        "    weight_other = 1.0           # Default weight for other cases\n",
        "\n",
        "    # Calculate weights based on y_true values\n",
        "    weights = tf.where(tf.equal(y_true, 1.0), weight_true_positive, weight_other)\n",
        "    weights = tf.where(tf.equal(y_true, 0.0), weight_false_negative, weights)\n",
        "\n",
        "    # Use BinaryCrossentropy as base loss\n",
        "    bce = tf.keras.losses.BinaryCrossentropy()\n",
        "    loss = bce(y_true, y_pred)\n",
        "\n",
        "    # Apply weighted loss\n",
        "    weighted_loss = loss * weights\n",
        "\n",
        "    # Reduce mean over all samples\n",
        "    return tf.reduce_mean(weighted_loss)\n",
        "\n",
        "# Parameters\n",
        "batch_size = 16\n",
        "num_classes = len(appliance_info_dict)\n",
        "sequence_length = 91\n",
        "num_features = 1\n",
        "\n",
        "# Data Generators\n",
        "data_generator = DataGenerator(appliance_info_dict, appliance_tuple, batch_size=batch_size)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Masking(mask_value=0., input_shape=(sequence_length, num_features)))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))  # Sigmoid activation for multi-label classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_weighted_loss, metrics=['binary_accuracy'])\n",
        "# loss='binary_crossentropy'\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint_filepath = 'model_checkpoint.h5'\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    save_freq='epoch',\n",
        "    period=1  # Save every 10 epochs\n",
        ")\n",
        "\n",
        "# Load the latest model checkpoint if available\n",
        "import os\n",
        "if os.path.exists(checkpoint_filepath):\n",
        "    model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "    print(\"Loaded model from checkpoint.\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "# Optional: Define a callback to print predictions during training\n",
        "def print_predictions_and_true(epoch, logs):\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"\\nPredictions and True Outputs at epoch\", epoch)\n",
        "        predictions = model.predict(data_generator)\n",
        "        true_outputs = next(iter(data_generator))[1]\n",
        "        print(\"Predictions:\")\n",
        "        print((predictions[:5] > 0.5).astype(int))\n",
        "        print(\"True Outputs:\")\n",
        "        print(true_outputs[:5])\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=print_predictions_and_true)\n",
        "\n",
        "# Training\n",
        "history = model.fit(data_generator, validation_data=data_generator, epochs=5, callbacks=[checkpoint_callback, print_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "T02eHkHmxGLG",
        "outputId": "34753d73-786d-4477-ef48-3db61daf412e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b77994bae7cf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/api/_v2/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/__internal__/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseRandomLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dense_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDenseAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtLarge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtSmall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/applications/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimagenet_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_preprocessing_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreprocessingLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Generic layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/testing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from pandas._testing import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0massert_extension_array_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0massert_frame_equal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m \u001b[0mcython_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cython_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming `data_generator` is your data generator\n",
        "# Assuming `model` is your trained model\n",
        "\n",
        "# Step 1: Make Predictions\n",
        "predictions = model.predict(data_generator)\n",
        "\n",
        "# Step 2: Collect True Labels\n",
        "true_labels = []\n",
        "for _, labels in data_generator:\n",
        "    true_labels.extend(labels)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Ensure predictions and true_labels have the same length\n",
        "assert len(predictions) == len(true_labels), \"Predictions and true labels must have the same length\"\n",
        "\n",
        "# Convert predictions to binary values (1 and 0)\n",
        "binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Print some of the binary predictions and true labels\n",
        "for i in range(10):\n",
        "    print(f\"Sample {i + 1}:\")\n",
        "    print(f\"Binary Prediction: {binary_predictions[i]}\")\n",
        "    print(f\"True Label: {true_labels[i]}\")\n",
        "    print('dishwasher, toilet_tm, sink_tm, toilet_km, shower_tm, shower_km, hose, laundry')\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Calculate reduced mean loss\n",
        "loss_fn = custom_weighted_loss\n",
        "loss = loss_fn(true_labels, predictions).numpy()\n",
        "print(f\"Reduced Mean Loss: {loss}\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(np.all(binary_predictions == true_labels, axis=1))\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Optionally, visualize the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Predictions\")\n",
        "plt.plot(predictions[:10], 'o')\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Prediction\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"True Labels\")\n",
        "plt.plot(true_labels[:10], 'o')\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zXt8kXsAHR8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}